<!DOCTYPE html>
<html lang = "en-US">
	<head>
		<title>math</title>
		<meta charset = "utf-8">
		<meta name = "description" content = "bread and education">
		<meta name = "keywords" content = "bread">
		<meta name = "author" content = "bread">

		<meta name = "viewport" content = "width = device-width, initial-scale = 1.0">
		<link rel = "icon" href = "../favicon.ico">
		<link rel = "stylesheet" href = "../styles.css" type = "text/css">
		<link rel = "stylesheet" href = "https://fonts.googleapis.com/css?family=Noto+Sans+JP|Noto+Sans+SC|Noto+Sans+TC|Share+Tech+Mono&display=swap&subset=chinese-simplified,chinese-traditional,japanese">
	</head>
	<body>
		<h1><a href = "../">bread</a> &#62; math</h1>
		<p><strong>Math</strong> is a fundamental language.</p>

		<nav>
		<ul>
			<li>1 <a href = "#quantity_types-of-numbers">Types of numbers</a><ul>
				<li>1.0 <a href = "#quantity_types-of-numbers_numbers">Numbers</a></li>
				<li>1.1 <a href = "#quantity_types-of-numbers_natural-numbers">Natural numbers (N)</a></li>
				<li>1.2 <a href = "#quantity_types-of-numbers_whole-numbers">Whole numbers (W)</a></li>
				<li>1.3 <a href = "#quantity_types-of-numbers_integers">Integers (Z)</a></li>
				<li>1.4 <a href = "#quantity_types-of-numbers_rational-numbers">Rational numbers (Q)</a></li>
				<li>1.5 <a href = "#quantity_types-of-numbers_real-numbers">Real numbers (R)</a></li>
				<li>1.6 <a href = "#quantity_types-of-numbers_classification-of-the-real-numbers">Classification of the real numbers</a></li>
				<li>1.7 <a href = "#quantity_types-of-numbers_numbers">Numbers</a></li>
				<li>1.0 <a href = "#quantity_types-of-numbers_numbers">Numbers</a></li>
				<li>1.0 <a href = "#quantity_types-of-numbers_numbers">Numbers</a></li>
				<li>1.0 <a href = "#quantity_types-of-numbers_complex-numbers">Complex numbers</a></li>
				<li>1.0 <a href = "#quantity_types-of-numbers_p-adic-numbers">p-adic numbers</a></li>
			</ul></li>
			<li>2 <a href = "#quantity_operations">Operations</a><ul>
				<li>2.0 <a href = "#quantity_operations_arity">Arity</a></li>
				<li>2.1 <a href = "#quantity_operations_hyperoperations">Hyperoperations</a>
					<ul>
						<li>2.1.1 <a href = "#quantity_operations_hyperoperations_succession-predecession">Succession / predecession</a></li>
						<li>2.1.2 <a href = "#quantity_operations_hyperoperations_addition-subtraction">Addition / subtraction</a></li>
						<li>2.1.3 <a href = "#quantity_operations_hyperoperations_multiplication-division">Multiplication / division</a></li>
						<li>2.1.4 <a href = "#quantity_operations_hyperoperations_exponentiation-logarithm">Exponentiation / logarithm</a></li>
						<li>2.1.5 <a href = "#quantity_operations_hyperoperations_generalizations">Generalizations</a></li>
					</ul>
				</li>
				<li>2.2 <a href = "#quantity_operations_other-operations">Other operations</a></li>
				<li>2.3 <a href = "#quantity_operations_order-of-operations">Order of operations</a></li>
				<!-- literally the end of pre-algebra -->
			</ul></li>
			<!-- number theory: factorization + fundamental theorem of arithmetic + primes, modular arithmetic, classification of the integers, number bases, etc., combinatorics (the study of counting), put graph theory in structures -->
		</ul>
		<a href = "#structure">Structure</a>
		<ul>
			<li>1 <a href = "#structure_elementary-algebra">Elementary algebra</a><ul>
				<li>1.0 <a href = "#structure_elementary-algebra_variables-relations-and-functions">Variables, relations, and functions</a></li>
				<li>1.1 <a href = "#structure_elementary-algebra_variables-relations-and-functions">Variables, relations, and functions</a></li>
				<!-- on sets, polynomials, conic sections, rationals, exponentials/logs, trig later, polar, parametric, multivariable later, tables and graphs, sequences, implicit -->
			</ul></li>
			<li>2 <a href = "#structure_linear-algebra">Linear algebra</a><ul>
				<li>2.0 <a href = "#structure_linear-algebra_mathematical-objects">Mathematical objects</a></li>
				<li>2.1 <a href = "#structure_linear-algebra_scalars-and-vectors">Scalars and vectors</a></li>
				<li>2.2 <a href = "#structure_linear-algebra_linear-systems">Linear systems</a></li>
				<li>2.3 <a href = "#structure_linear-algebra_mathematical-objects">Mathematical objects</a></li>
			</ul></li>
			<li>3 <a href = "#structure_multilinear-algebra">Multilinear algebra</a></li>
			<li>4 <a href = "#structure_abstract-algebra">Abstract algebra</a></li>
			<li>5 <a href = "#structure_group-theory">Group theory</a></li>
		</ul>
		<a href = "#space">Space</a>
		<ul>
			<li>0 <a href = "#space_dimension-measure-and-metric">Dimension, measure, and metric</a></li>
			<li>1 <a href = "#space_geometry">Geometry</a></li>
			<li>2 <a href = "#space_trigonometry">Trigonometry</a></li>
			<li>3 <a href = "#space_topology">Topology</a></li>
		</ul>
		<a href = "#change">Change</a>
		<ul>
			<li>0 <a href = "#change_functions">Functions</a></li>
			<li>1 <a href = "#change_limits-and-continuity">Limits and continuity</a><ul>
				<li>1.0 <a href = "#change_limits-and-continuity_intuition-for-limits">Intuition for limits</a></li>
				<li>1.1 <a href = "#change_limits-and-continuity_properties-of-limits">Properties of limits</a></li>
				<li>1.2 <a href = "#change_limits-and-continuity_types-of-limits">Types of limits</a></li>
				<li>1.3 <a href = "#change_limits-and-continuity_evaluation-of-limits">Evaluation of limits</a></li>
				<li>1.4 <a href = "#change_limits-and-continuity_continuity">Continuity</a></li> <!-- make joks about it being hard to spell -->
				<li>1.5 <a href = "#change_limits-and-continuity_discontinuities">Discontinuities</a></li>
			</ul></li>
			<li>2 <a href = "#change_differential-calculus">Differential calculus</a><ul>
				<li>2.0 <a href = "#change_differential-calculus_differentials">Differentials</a></li>
				<li>2.1 <a href = "#change_differential-calculus_definition-of-the-derivative">Definition of the derivative</a></li>
				<li>2.2 <a href = "#change_differential-calculus_rules">Rules</a></li>
				<li>2.3 <a href = "#change_differential-calculus_differentiability">Differentiability</a></li>
				<li>2.4 <a href = "#change_differential-calculus_existence-theorems">Existence theorems</a></li>
				<li>2.5 <a href = "#change_differential-calculus_analysis-of-functions">Analysis of functions</a></li>
				<li>2.6 <a href = "#change_differential-calculus_other-applications">Other applications</a></li>
			</ul></li>
			<li>3 <a href = "#change_integral-calculus">Integral calculus</a><ul>
				<li>3.1 <a href = "#indefinite">Indefinite integrals</a></li>
				<li>3.2 <a href = "#rules">Rules</a></li>
				<li>3.3 <a href = "#techniques">Techniques</a></li>
				<li>3.4 <a href = "#area">Area</a></li>
				<li>3.5 <a href = "#definite">Definite integrals</a></li>
				<li>3.6 <a href = "#properties">Properties</a></li>
			</ul></li>
			<li>4 <a href = "#change_fundamental-theorem-of-calculus">Fundamental theorem of calculus</a></li>
			<li>5 <a href = "#change_series-calculus">Series calculus</a></li>
			<li>6 <a href = "#change_vector-calculus">Vector calculus</a></li>
			<li>7 <a href = "#change_multivariable-calculus">Multivariable calculus</a></li>
			<li>8 <a href = "#change_differential-equations">Differential equations</a></li>
			<li>9 <a href = "#change_real-analysis">Real analysis</a></li> <!-- rigor -->
			<li>10 <a href = "#change_complex-analysis">Complex analysis</a></li>
			<li>11 <a href = "#change_harmonic-analysis">Harmonic analysis</a></li>
			<li>12 <a href = "#change_functional-analysis">Functional analysis</a></li>
			<li>13 <a href = "#change_dynamical-systems">Dynamical systems</a></li> <!-- math models -->
			<li>14 <a href = "#change_chaos-theory">Chaos theory</a></li>
		</ul>
		<a href = "#foundations">Foundations</a>
		<ul>
			<li>0 <a href = "#space_dimension-measure-and-metric">Dimension, measure, and metric</a></li>
			<li>1 <a href = "#space_geometry">Geometry</a></li>
			<li>2 <a href = "#space_trigonometry">Trigonometry</a></li>
			<li>3 <a href = "#space_topology">Topology</a></li>
		</ul>
		<a href = "#applied">Applied</a>
		<ul>
			<li>1 <a href = "#applied_probability">Probability</a><ul>
				
			</ul></li>
			<li>2 <a href = "#applied_statistics">Statistics</a><ul>
				<li>2.1 <a href = "#applied_statistics_descriptive-statistics">Descriptive statistics</a><ul>
					<li>2.1.0 <a href = "#applied_statistics_descriptive-statistics_data">Data</a></li>
					<li>2.1.1 <a href = "#applied_statistics_descriptive-statistics_distributions">Distributions</a></li>
					<li>2.1.2 <a href = "#applied_statistics_descriptive-statistics_displaying-data">Displaying data</a></li>
					<li>2.1.3 <a href = "#applied_statistics_descriptive-statistics_modeling-distributions">Modeling distributions</a></li>
					<li>2.1.4 <a href = "#applied_statistics_descriptive-statistics_random-variables">Random variables</a></li>
					<li>2.1.5 <a href = "#applied_statistics_descriptive-statistics_relationships">Relationships</a></li>
				</ul></li>
				<li>2.2 <a href = "#applied_statistics_data-collection">Data collection</a><ul>
					<li>2.2.1 <a href = "#applied_statistics_data-collection_observational-studies">Observational studies</a></li>
					<li>2.2.2 <a href = "#applied_statistics_data-collection_experiments">Experiments</a></li>
					<li>2.2.3 <a href = "#applied_statistics_data-collection_misuse">Misuse</a></li>
				</ul></li>
				<li>2.3 <a href = "#applied_statistics_inferential-statistics">Inferential statistics</a><ul>
					<li>2.3.0 <a href = "#applied_statistics_inferential-statistics_probability">Probability</a></li>
					<!-- frequentist vs. bayesian -->
					<li>2.3.1 <a href = "#applied_statistics_inferential-statistics_random-variables">Random variables</li>
					<li>2.3.2 <a href = "#applied_statistics_inferential-statistics_sampling-distributions">Sampling distributions</a></li>
					<li>2.3.3 <a href = "#applied_statistics_inferential-statistics_point-estimation">Point estimation</a></li>
					<li>2.3.4 <a href = "#applied_statistics_inferential-statistics_interval-estimation">Interval estimation</a></li>
					<li>2.3.5 <a href = "#applied_statistics_inferential-statistics_testing-hypotheses">Testing hypotheses</a></li>
					<li>2.3.6 <a href = "#applied_statistics_inferential-statistics_comparing-groups">Comparing groups</a></li>
					<li>2.3.7 <a href = "#applied_statistics_inferential-statistics_categorical-inference">Categorical inference</a></li>
					<li>2.3.8 <a href = "#applied_statistics_inferential-statistics_bayesian-inference">Bayesian inference</a></li>
				</ul></li>
				<li>2.4 <a href = "#applied_statistics_correlation-and-regression-analysis">Correlation and regression analysis</a>
				
				</li>
				<li>2.5 <a href = "#applied_statistics_other-analyses">Other analyses</a><ul>
					<li>2.5.1 <a href = "#applied_statistics_other-analyses_categorical-analysis">Categorical analysis</a></li>
					<li>2.5.2 <a href = "#applied_statistics_other-analyses_multivariate-analysis">Multivariate analysis</a></li>
					<li>2.5.3 <a href = "#applied_statistics_other-analyses_time-series-analysis">Time-series analysis</a></li>
					<li>2.5.4 <a href = "#applied_statistics_other-analyses_survival-analysis">Survival analysis</a></li>
				</ul></li>
			</ul></li>
		</ul>
		</nav>
		<p>
			The name "applied math" is extremely misleading, as it suggests that all other math is just "pure math", unconnected to the real world. It is better to say that "applied math" specifically refers to math applied in other fields, like science or economics.
		</p>
		<br>
		<h2 id = "applied_statistics">2 <a href = "#applied">Applied</a> &#62; Statistics</h2>
		<p>
			<strong>Statistics</strong> is the study of data. Every statistics problem involves four steps: the formulation of a question, the collection of data, the organization and representation of data, and the interpretation of data. While it is often considered a separate "decision science", it is so closely related to math that I choose to present it as an applied math.
		</p>
		<h2 id = "applied_statistics_descriptive-statistics">2.1 <a href = "#applied_statistics">Statistics</a> &#62; Descriptive statistics</h2>
		<p>
			Descriptive statistics aims to describe data.
		</p>
		<h3 id = "applied_statistics_descriptive-statistics_data">2.1.0 Data</h3>
		<p>
			<strong>Data</strong> is simply new information with context.
		</p><p>
			A <strong>data set</strong> is, well, a set of data, and consists of individuals and variables.
		</p><p>
			<strong>Individuals</strong> (also cases or observations) are the things described by the data. The <strong>sample</strong> is the set of individuals that you have collected data on, and the <strong>population</strong> is the set of individuals you are trying to investigate. Every individual is an element of the sample, and the sample is a subset of the population.
		</p><p>
			<strong>Variables</strong> are characteristics of an individual whose values can vary from individual to individual. There are two types of variables, categorical (or qualitative) and quantitative. <strong>Categorical variables</strong> take on values that can be divided into categories, like brand or color, whereas <strong>quantitative variables</strong> take on numerical values. It is important to note the distinction between the two types of variables; variables like zip code or address are numerical but not quantitative, because it doesn't make sense to treat a zip code or address like a number.
		</p><p>
			Quantitative variables can be further classified into two types, continuous and discrete; a <strong>continuous quantitative variable</strong> can take on every value between its minimum and maximum, like height, whereas a <strong>discrete quantitative variable</strong> can only take on certain values, like age (whole numbers).
		</p>
		<img src = "image/matha_stats_summary1" alt = "[A data set consists of individuals, which are part of the sample and population, and variables, which are either categorical or quantitative.]">
		
		<h3 id = "applied_statistics_descriptive-statistics_distributions">2.1.1 Distributions</h3>
		<p>
			Describing data by just showing all of the numbers works, if you only have, say, 20 individuals. Any more than that, however, and we'll need something more concise to describe variables.
		</p><p>
			The <strong>distribution</strong> of a variable describes how often what values a variable takes and how often it takes them. The values taken are simply the categories or numbers of the variables. The measure of "how often" can be described in two ways. <strong>Frequency</strong> is a fancy name for count, so if one million people like garlic bread, the frequency of people that like garlic bread is one million. <strong>Relative frequency</strong> (<strong>proportion</strong>) is a fancy name for percent, so if 100% of people like garlic bread, the relative frequency of people that like garlic bread is 100% (as it should be).
		</p><p>
			If we plot the values on the horizontal axis and the frequencies on the vertical axis, we get a graph of a distribution.
		</p>
		<img>
		<p>
			There are four key features in any distribution: shape, center, spread, and outliers.
		</p>
		<h4>Shape</h4>
		<p>
			<strong>Shape</strong> requires you to describe, in words, what you see on the graph. You'd typically mention things like major peaks (also called modes), clusters of values, obvious gaps, and outliers. <strong>Symmetry</strong> occurs if the left side and right side are mirror images, while <strong>skewness</strong> to the left or right occurs if the left side or right side has a longer tail. <strong>Unimodal</strong> distributions have one clear peak, <strong>bimodal</strong> distributions have two clear peaks, and <strong>multimodal</strong> distributions have multiple clear peaks.
		</p>
		<h4>Center</h4>
		<p>
			<strong>Center</strong> (also measure of center or central tendency) is a number that describes the center value or typical value.
		</p><p>
			<strong>Mode</strong>, which is the value that occurs the most often (corresponding to a global peak on a graph), is a measure of center, although used less often.
		</p><p>
			<strong>Mean</strong> actually refers to many different measures of center. The one we typically call "mean" or "average" is the <strong>arithmetic mean</strong>, which is also the most common measure of center. The arithmetic mean (AM) can be interpreted as the center of mass of a distribution, the point that balances the distribution. The arithmetic mean is also the value that every individual would have if the total was evenly distributed among everyone. Computationally, the mean is the sum of all values divided by the number of values, or <span>(&#931; x_i) / n</span> (in a set of n observations). The arithmetic mean of a sample is written as <span>x&#772;</span> (x-bar), while the population mean is written <span>&#956;</span> (mu).
		</p><p>
			The <strong>geometric mean</strong> (GM) is the nth root of the product of all values, or <span>GM = (&#928; x_i) ^ (1 / n)</span>, useful when values represents rates of change; for example, the GM of 1, 3, and 9 is 3. The <strong>harmonic mean</strong> (HM) is the harmonic (reciprocal) of an arithmetic mean of harmonic values (reciprocal values); i.e., <span>HM = [AM((x_1)^(-1), (x_2)^(-1), ..., (x_n)^(-1))]^(-1)</span>. The harmonic mean is useful for adding rates or when the values are defined as the harmonic of some unit; for example, speed is distance with respect to the reciprocal of time.
		</p><p>
			The three means, AM, GM, and HM together are called the <strong>Pythagorean means</strong>, and they always satisfy the property that <span>max >= AM >= GM >= HM >= min</span>, with equality only if every value is equal.
		</p><p>
			The <strong>weighted arithmetic mean</strong> applies when different values have different influences on the average. If we let w be the influence (or weight) of a value, then the weighted mean is <span>(&#931; (w * x)) / (&#931; w)</span>. Examples of the weighted mean include the calculation for a center of mass or combining samples of different sizes. An arithmetic mean is just a weighted arithmetic mean with all equal weights.
		</p><p>
			The <strong>truncated mean</strong> is the mean of a sample with some values removed from it. An example is the <strong>interquartile mean</strong> (IQM), which is simply the mean of the middle 50% of values.
		</p><p>
			The <strong>moving average</strong> is a changing mean that is calculated every time new values are available, such as with stocks.
		</p><p>
			At this point, we could cover the power mean and the generalized f-mean, but we have already talked about way too many means than necessary. From this point forward, "mean" will only refer to "arithmetic mean". 	
		</p><p>
			<strong>Median</strong> (<span>M</span>) is a measure of center that corresponds to the midpoint of the distribution, such that about half of the values are lower and half of the values are higher than it. To find the median, we need a sorted list; if there are an odd number of observations, the median is simply the center value; if there are an even number of observations, the median is defined as the average of the two center values.
		</p><p>
			In an exactly symmetric distribution, the mean and median are the same. In a roughly symmetric distribution, the mean and median are close together. In a skewed distribution, the mean is usually farther out along the tail than the median. 
		</p>
		<h4>Spread</h4>
		<p>
			<strong>Spread</strong> (also dispersion, scatter, or variability) is another number which describes how spread out the data values are.
		</p><p>
			 Measures of center have the same units as the values they describe, whereas measure of spread may have the same units, squared units, or be dimensionless. A key feature of measures of spread is that they are nonnegative, being zero when all data values are the same and increasing as the data values are more spread out.
		</p><p>
			<strong>Range</strong> is the simplest measure of spread, defined as <span>range = max - min</span>.
		</p><p>
			<strong>Interquartile range</strong> (<span>IQR</span>) is a measure of spread defined as the range of the middle 50% of values. To find the middle 50% of values, we must first divide the data set into quarters. The median (also the <strong>second quartile Q2</strong>) divides the data set in half. The <strong>first quartile</strong> (<span>Q1</span>) is the median of all of the observations to the left of the median. The <strong>third quartile</strong> (<span>Q3</span>) is the median of all observations to the right of the median. IQR is defined as <span>IQR = Q3 - Q1</span>.
		</p><p>
			<strong>Standard deviation</strong> (<span>s_x, s, or SD</span>) measures the typical distance of a value from the mean.
		</p><p>
			A <strong>deviation</strong> is just a displacement of a value from the mean (<span>x_i - x-bar</span>). It's only natural to assume that the standard deviation is just the average deviation, but it turns out that deviations can cancel out (and the average deviation from the mean will always be zero). To solve this problem, statisticians choose to use squared deviations to ensure that the value is nonnegative. The average of squared deviations is known as the <strong>variance</strong>, denoted by <span>(s_x)^2, s^2, or Var(x)</span>, and the standard deviation is the square root of the variance, because variance has squared units relative to the original values, whereas standard deviation has the same units.
		</p><p>
			It turns out that the computation of variance and standard deviation are also designed to be confusing (but still work). A squared deviation is <span>(x_i - x-bar)^2</span>, so the variance, the average of squared deviations, must be <span>(1 / n) * &#931; (x_i - x-bar)^2</span>, right? Well, the variance is actually defined as <span>(1 / (n - 1)) * &#931; (x_i - x-bar)^2</span>, not to confuse people (ok, maybe partially to confuse people) but mainly so it works for all n.
		</p><p>
			The population standard deviation and variance are denoted <span>&#963;</span> (sigma) and <span>&#963;^2 </span> respectively, and they actually use the expected method of calculation, dividing by n rather than (n - 1).
		</p><p>
			There are actually more intuitive measures of spread than standard deviation. Remember when we chose to use the square root of the average squared deviation for some reason? We could have also simply used the <strong>average absolute deviation</strong>, which is exactly what the name suggests: the average of the absolute value of the deviations <span>|x_i - x-bar|</span>.
		</p><p>
			There is also a <strong>relative standard deviation</strong> (also the <strong>coefficient of variation</strong> <span>CV</span>), where <span>c_v = &#963; / &#956;</span>. This measure should only be used for absolute units (like Kelvin), because it is rather meaningless if the mean can be close to zero. This measure is sometimes better than standard deviation in that it is dimensionless, measuring the standard deviation against the mean, so different sets of data could be compared.
		</p><p>
			It should be noted that calculated measures of center and spread apply only to quantitative distributions, because it's a bit difficult to do calculations with qualities.
		</p>
		<h4>Outliers</h4>
		<p>
			<strong>Outliers</strong> are values that deviate greatly from the pattern of a distribution. Graphically, they are values which are far apart from a majority of values. Numerically, the <strong>1.5 * IQR rule</strong> defines outliers as observations that are at least <span>1.5 * IQR</span> less than <span>Q1</span> or <span>1.5 * IQR</span> greater than <span>Q3</span>.
		</p><p>
			<strong>Resistant measures</strong> are measures that are not affected by extreme observations like outliers. Resistant measures are quartiles (Q1, median, and Q3) and IQR, while nonresistant measures are mean, range, standard deviation, and variance. Typically, median and IQR are paired together to better describe skewed distributions or ones with strong outliers, while mean and standard deviation are paired together to describe reasonably symmetric distributions without outliers.
		</p>
		<h4>Location</h4>
		<p>
			<strong>Location</strong> (position) is a generalization of the idea of center, although it is applied to specific values in the data set. The most common measure of location is using percentiles; the pth <span>percentile</span> of a distribution is the value with p percent of the observations less than it. There are many definitions of percentiles, but I will choose the one such that the minimum is the 0th percentile, Q1 is about the 25th percentile, the median M (Q2) is about the 50th percentile, Q3 is about the 75th percentile, and the maximum is the 99th percentile.
		</p><p>
			<strong>Standardized scores (z-scores)</strong> are another way of representing location; the z-score of a value x is <span>z = (x - mean) / (standard deviation)</span>. Standardizing a score translates it into a dimensionless unit in terms of standard deviations, measuring how many standard deviations a number is from the mean, which also allows us to compare location in different distributions.
		</p><p>
			In summary, distributions contain values and frequencies; the four key features of distributions are shape, center, spread, and outliers; we can also measure location in a distribution.
		</p>

		<h3 id = "applied_statistics_descriptive-statistics_displaying-data">2.1.2 Displaying data</h3>
		<p>
			Generally, data is displayed in three ways: tables, graphs / charts / plots, or numerical summaries. Tables are only useful for smaller sets of data, numerical summaries do not give a complete picture, while graphs are easy to view and interpret, so graphs should always be included with a data set.
		</p>
		<h4>Tables</h4>
		<p>
			The simplest table just includes all of the raw data, where each row represents an individual and each column represents a variable. While this is simple and lazy, it becomes difficult to view for any large set of data.
		</p><p>
			<strong>Frequency tables</strong> and <strong>relative frequency tables</strong> are essentially summaries of a distribution, with the values of the variable displayed in one column and the frequencies or relative frequencies in the other column.
		</p><p>
			<strong>Cumulative frequency tables</strong> and <strong>cumulative relative frequency tables</strong> are similar to frequency tables, except the cumulative frequency or cumulative relative frequency for each value is the frequency or relative frequency of that value and all values smaller than it.
		</p><p>
			<strong>Two-way tables</strong> display two categorical variables, called the row variable and column variable, where each entry is the intersection of a value of the row variable and a value of the column variable.
		</p><p>
			A <strong>marginal distribution</strong> is the distribution of a single categorical variable for all the individuals in a two-way table. It's called a marginal distribution because, well, it appears in the margins. You can identify it as the total row or total column in a two-way table.
		</p><p>
			A <strong>conditional distribution</strong> is the distribution of a single categorical variable for all the individuals with a specific condition in a two way table. You can identify one as any row or column in a two-way table. There are only two marginal distributions in a two-way table, one for each variable, but there are two sets of conditional distributions, one fore the set of all rows and one for the set of all columns. Conditional distributions, unlike marginal distributions, can help identify the relationship between two categorical variables.
		</p>
		<h4>Graphs, charts, plots, or whatever you want to call them</h4>
		<p>
			I made up this theorem, but I felt like it was important enough to have its own name:
		</p><p>
			<strong>The fundamental theorem of statistical graphs</strong> states that our eyes interpret area as values or proportions.
		</p><p>
			The first consequence of this "theorem" is that graphs should have intervals of equal width if height varies and intervals of equal heights if width varies, to ensure that proportions are presented accurately. So, avoid pictographs.
		</p><p>
			The second consequence of this "theorem" is that the truncation and scaling of axes can change our perception of graphs, often making them misleading.
		</p><p>
			The third consequence of this "theorem" is that in order to get a value or proportion from a continuous graph, we will require a way to describe continuous area, which is in the form of integrals.
		</p>
		<h4>For categorical variables</h4>
		<p>
			The <strong>pie chart</strong> displays a categorical variable, where a circular pie represents the entire variable and each category is a slice of pie, sized according to its proportion to the whole. Pie charts are a common but rather bad way of displaying data, because they are difficult to make by hand, difficult to read if not labeled, difficult to make if categories are small, and applicable in only one specific case: when you want to show each category in relation to the whole.
		</p><p>
			The <strong>bar graph</strong> is a more flexible way of displaying a categorical variable, with categories on the horizontal axis and frequencies or relative frequencies on the vertical axis. Each category is represented by a bar, with equal width and a height corresponding to its frequency or relative frequency.
		</p><p>
			The <strong>side-by-side bar graph</strong> is simply a combination of two or more bar graphs, where the bars for each category are placed side-by-side, which makes it ideal to compare bar graphs (categorical variables). 
		</p><p>
			The <strong>segmented bar graph</strong> represents each categorical distribution as a single bar of height 100% (or the total number), which is divided into categories; essentially, it takes a bar graphs and stacks all of the bars on top of each other. You can compare two categorical variables using this, but it is difficult to tell the values of individual categories, so side-by-side bar graphs work better.
		</p>
		<h4>For quantitative variables</h4>
		<p>
			The <strong>dotplot</strong> displays a quantitative variable by showing each value as a dot, above its location on a number line. It is simple to make and interpret, shows all data values, shows all four features of a distribution, and easily comparable; it's perhaps only limited when data values are extremely spread out.
		</p><p>
			The <strong>stemplot</strong> (stem-and-leaf plot) displays a quantitative variable by brutally mutilating every data value. Each data value is separated into its final digit, the leaf, and the other numbers, the stem. The stems are arranged on the left in increasing order, without skipping any stems in between (even if there's no values for a stem), and the leafs are arranged in increasing order to the right of its stem; a key is necessary to explain how it works in context. It's certainly a bit more contrived than a dotplot, but it surprisingly does show all of the raw data and shows the four features of a distribution.
		</p><p>
			The <strong>back-to-back stemplot</strong> compares two quantitative variables; using common stems, the leafs of one variable are arranged on the right and another on the left.
		</p><p>
			<strong>Splitting stems</strong> allows for more flexibility in a stemplot, by dividing a stem into multiple parts, often two stems (each with five possible leafs) or five stems (each with two possible leafs). When the values have too many digits, rounding or truncating the values can make stemplots more flexible. However, stemplots are still relatively limited; they don't work well for large data sets, and they don't show the shape of the distribution well for too little or too many stems (5-10 is a good number of stems).
		</p><p>
			<strong>Histograms</strong> are a more graphical way of displaying quantitative variables. The data is divided into classes of equal width, where each class is represented by a bar, and the frequency or relative frequency of each class is plotted vertically; essentially, we make categories out of quantitative data to plot it like a bar graph. However, you should not confuse histograms and bar graphs; histograms display quantitative data, so its horizontal axis contains numerical values, and its bars are connected, whereas bar graphs display categorical data, so its horizontal axis contains categories, and its bars are separated (to show the fact that there are separate categories). As always, choose an appropriate number of classes (5-10 is a good number of classes) to show the shape of the distribution, and use relative frequencies when comparing two histograms' distributions.
		</p><p>
			A <strong>cumulative relative frequency graph<strong> (also known as "ogives" for some reason) is actually a graph of location, which plots quantitative values on the horizontal axis and percentiles on the vertical axis. It is constructed from a relative frequency table, starting at 0 at the minimum and ending at 100 at the maximum value, with consecutive points connected by line segments. Any individual point on the cumulative relative frequency graph represents the approximate position and percentile of a single individual.
		</p><p>
			Why are there so many types of graphs? Well, each graph has its advantages and disadvantages, so that you can choose which graph displays a distribution meaningfully for a data set.
		</p>
		<h4>Numerical summaries</h4>
		<p>
			Numerical summaries attempt to summarize the data using only a few important numbers; a useful summary contains, at the minimum, a measure of center and a measure of spread (often the mean and standard deviation). However, they are never a full description of the data, so graphs of the distribution should always be provided.
		</p><p>
			The <strong>five-number summary</strong> consists of the minimum, Q1, median (Q2), Q3, and maximum.
		</p><p>
			The <strong>boxplot</strong> (box-and-whiskers plot) displays the five-number summary, which makes it not a full graphical description of the distribution. There are actually two types of boxplots, the one with outliers and the one without outliers (I couldn't find official names for these). For both types of boxplots, a central box is drawn from Q1 to Q3, with a line in the box marking the median. Lines (whiskers) extend from the box, either to the minimum and maximum, or to the smallest and largest observations that are non-outliers (and outliers are marked with an asterisk). 
		</p><p>
			<strong>Side-by-side boxplots</strong> compare two boxplots by placing them side-by-side. Because boxplots are less detailed, they are actually best used for comparison. We can still tell some information about the distribution from a boxplot: shape (symmetry and skewness can be seen from the distance of the quartiles and extrema to the median), center (the median), spread (range and the size of the box), and outliers (if shown).
		</p>
		<img>
		
		<h3 id = "applied_statistics_descriptive-statistics_modeling-distributions">2.1.3 Modeling distributions</h3>
		<p>
			Many times we find that the distribution of a quantitative variable follows a certain pattern, so it may be useful to describe it using a mathematical model.
		</p>
		<h4>Transforming distributions</h4>
		<p>
			<span>Transforming data</span> becomes easier when we visualize the distribution as if it were a function.
		</p><p>
			If we add a constant c to all values in our data set, we essentially shift the distribution c units left (if c is negative) or right (if c is positive). Obviously, just moving the distribution around doesn't change the shape or spread; it merely adds c to the measures of center and location.
		</p><p>
			If we multiply a constant c to all values in our data set, we essentially horizontally stretch (c > 1) or compress (1 > c > 0) our distribution. The shape still wouldn't change, unless c was negative, in which case the distribution would be reflected horizontally. Measures of center and location would simply all be multiplied by c. Measures of spread would be multiplied by |c|, because spread is always nonnegative (note that only measures of spread with the same dimension will be multiplied by c, whereas others like variance are multiplied by c^2 or remain constant).
		</p><p>
			Successively applying transformations to a distribution, like <span>ax + b</span>, simply applies the effects one after another (in this case, multiplying by a then adding b).
		</p><p>
			Standardizing scores (z-scores) is a form of transformation that maintains the shape of the distribution, but converts the standard deviation to 1 and the mean to 0 for all distributions.
		</p>
		<h4>Density curves</h4>
		<p>
			<span>Density curves</span> are the models we use to describe the overall pattern of a distribution. It is a smooth function f(x), where x is the values and f is the frequencies, with two properties: <span>f(x) >= 0 for all x</span>, and the <span>integral [from -&#8734; to &#8734;] f(x) dx is 1</span>. The first property states that f(x) describes frequency, and is thus never negative. The second property states that the total area under the curve of f(x) should be 1, which represents the total proportion.
		</p>
		<p>
			Because we respond to area as proportion, the integral (area) over any interval is equivalent to the proportion (relative frequency) of all values that fall in that interval; i.e., <span>proportion of values in [a, b] = integral [from a to b] f(x) dx</span>. Note that the proportion of any single value c is 0, as the <span>integral [from c to c] = 0</span>.
		</p><p>
			The mode of a density curve is simply <span>max(f(x))</span>. The median M of a density curve is the point which divides the curve into two equal areas; i.e. <span>integral [from -&#8734; to M] f(x) dx = integral [from M to &#8734;] f(x) dx = 1 / 2</span>. The mean is the center of mass <span>&#956; = integral [from -&#8734; to &#8734;] xf(x) dx</span>, essentially a weighted mean of all of the values. Notice that the mean of a density curve is denoted &#956;, as it is an idealistic estimation of the actual x-bar.
		</p><p>
			The variance of a density curve is essentially a weighted mean of the squared deviations, &#963;^2 = integral [from -&#8734; to &#8734;] (x - mu)^2 f(x) dx, and the standard deviation &#963; is still the square root of variance.
		</p>
		<img>
		<h4>Normal curves</h4>
		<p>
			<span>Normal curves</span> (also Gaussian distributions or bell curves) are, contrary to their name, extremely special density curves. They arise naturally with many types of data and most chance processes, being the distribution which maximizes entropy, and they form the foundation for inferential statistics. The shape of any normal curve is symmetric, unimodal, and bell-shaped.
		</p><p>
			Any normal curve is completely described by giving its mean &#956; and variance &#963;^2 (or mean and standard deviation). The equation for a normal curve is N(&#956;, &#963;^2) = f(x | &#956;, &#963;^2) = (1 / sqrt(2&#960; * &#963;^2)) * exp[-(x - &#956;)^2 / (2&#963;^2)]. The mean, median, mode, center, and peak are all the same value &#956;. The standard deviation &#963; is the distance from the mean to the inflection points of the curve.
		</p><p>
			<span>The 68-95-99.7 rule</span> (empirical rule) states that for any normal distribution, about 68% of values are within &#963; of &#956;, about 95% of values are within 2&#963; of &#956;, and about 99.7% of values are within 3&#963; of &#956;.
		</p><p>
			<span>Chebyshev's inequality</span> is a more general rule for any distribution, which states that the proportion of observations that fall within k standard deviations of the mean is at least 1 - k^(-2).
		</p><p>
			The <span>standard normal distribution</span> is N(&#956; = 0, &#963;^2 = 1) = (1 / sqrt(2&#960;)) * exp[-x^2 / 2]. The distribution of standardized scores for any normal distribution is the standard normal distribution.
		</p><p>
			We can determine the normality of a distribution by first looking for lack of skewness, unimodality, and bell-shaped graphs, then using the 68-95-99.7 rule, then using <span>normal probability plots</span>. Normal probability plots plot the x-values against the expected z-values (using percentiles on a standard normal distribution); a linear plot indicates a normal distribution, while systematic deviations indicate non-normality. If the largest observations fall to the right of a line drawn through the main body of points, the distribution is right-skewed; if the smallest observations fall to the left, the distribution is left-skewed.
		</p>

		<h3 id = "applied_statistics_descriptive-statistics_random-variables">2.1.4 Random variables</h3>
		<p>
			<strong>Random variables</strong> are quantitative variables that take numerical values which describe the outcomes of some chance process.
		</p><p>
			<strong>Probability distributions</strong> are the distributions for random variables, which lists all possible values and their probabilities (rather than frequencies).
		</p><p>
			<strong>Discrete random variables</strong> take a discrete set of possible values. <strong>Continuous random variables</strong> take all values within an interval.
		</p><p>
			The probability distributions of discrete random variables and lists their values <span>x_1, x_2, x_3, ...</span> and probabilities <span>p_1, p_2, p_3, ...</span>. The probabilities <span>p_i</span> are all valid probabilities between 0 and 1, which sum to 1; the probability of any event is the sum of the probabilities of the values that make up the event.
		</p><p>
			The probability distributions of random variables are described by histograms (which are just discrete density curves) and density curves (which are just continuous histograms).
		</p><p>
		The <strong>expected value</strong> of a discrete random variable 
		</p><p>
		
		</p>
		
		<h3 id = "applied_statistics_descriptive-statistics_categorical-variables">2.1.7 Relationships</h3>
		<p>
			<strong>Response variables</strong> (<strong>dependent variables</strong>) measures the outcome of a study, while <strong>explanatory variables</strong> (<strong>independent variables</strong>) may help explain or predict changes in a response variable.
		</p><p>
			In a scatterplot, each individual appears as a point (x, y), where x is its value of the explanatory variable and y is its value of the response variable (if there are explanatory or response variables). 
		</p><p>
	There is an association between two variables if knowing the value of one variable helps predict the value of the other.
			<strong>Positive association
		</p>
		
		<h2 id = "applied_statistics_data-collection">2.2 <a href = "applied_statistics">Statistics</a> &#62; Data collection</h2>
		<p>
			Data collection investigates the collection of data (surprising, I know). This section technically has nothing to do with math, but it's still important if you don't want terrible data.
		</p>
		<h3 id = "applied_statistics_data-collection_observational-studies">2.2.1 Observational studies</h3>
		<p>
			<strong>Observational studies</strong> are studies that observe individuals, measuring the variables of interest, but do not attempt to influence the responses.
		</p><p>
			A <strong>census</strong> collects data on the entire population. Obviously, this is rather difficult, so a <strong>sample survey</strong> is more commonly used, which chooses a representative sample from the population. Note that a survey doesn't actually have to ask questions to individuals.
		</p><p>
			<strong>Bias</strong> occurs if the study design consistently produces underestimations or overestimations of the value you want to know. Obviously, bias is bad.
		</p>
		<h4>Sampling methods</h4>
		<p>
			<strong>Convenience sampling</strong> chooses individuals that are easy to reach, which produces bias, which is bad.
		</p><p>
			<strong>Voluntary response sampling</strong> (self-selected sampling) consists of individuals who choose themselves by responding to a general invitation, which produces bias, which is bad.
		</p><p>
			<strong>Random sampling</strong> uses a chance process to determine which individuals in the population are included in the sample, which avoids bias, which is good.
		</p><p>
			<strong>Simple random sampling</strong> (SRS) is done in a way such that every group of n individuals in the population has an equal chance of being the sample of size n. Choosing an SRS may involve dice or drawing names out of a hat, but typically involves labelling each individual and generating random numbers via a table or technology.
		</p><p>
			<strong>Stratified random sampling</strong> involves first dividing the population into groups of similar individuals, known as <strong>strata</strong>, then separately taking an SRS in each stratum and combining them to form the sample. Strata are often chosen using facts known beforehand, and work best when individuals are very similar within each strata but largely different across strata.
		</p><p>
			<strong>Cluster sampling</strong> involves first dividing the population into groups of individuals located near each other, known as <strong>clusters</strong>, then taking an SRS of the clusters (or separately taking an SRS within each cluster) and combining them to form the sample. Clusters are often chosen for ease, and work best when individuals are very different within each cluster but largely similar across clusters (i.e., clusters resemble a smaller version of the population).
		</p><p>
			<strong>Multistage sampling</strong> combines two or more sampling methods. Strata, cluster, and multistage sampling are all more time-efficient than simple random sampling.
		</p>
		<img>
		<h4>Survey problems</h4>
		<p>
			Random sampling can avoid bias, but there are other problems that are more difficult to avoid.
		</p><p>
			<strong>Undercoverage</strong> occurs when some individuals in the population cannot be chosen in a sample, sometimes due to an incomplete sampling frame (the list of individuals from which a sample is chosen).
		</p><p>
			<strong>Nonresponse</strong> occurs when some an individual chosen for the sample cannot be reached or refuses to participate; this is a serious source of bias, with nonresponse rates often exceeding 50%.
		</p><p>
			<strong>Response bias</strong> is a systematic pattern of inaccurate answers, whether due to individuals or interviewers.
		</p><p>
			<strong>Question wording</strong> (and question order) is actually the most important factor on the answers given to a sample survey.
		</p>
		
		<h3 id = "applied_statistics_data-collection_experiments">2.2.2 Experiments</h3>
		<p>
			<strong>Experiments</strong> are studies that deliberately impose some treatment on individuals to measure their response.
		</p><p>
			Observational studies aim to describe some group or situation, compare groups, or examine the relationship between variables. Experiments aim to determine whether the treatment causes changes to the response variable. If you wanted to reliably determine cause-and-effect, experiments should be used.
		</p><p>
			<strong>Confounding</strong> occurs when two variables are associated in such a way that their effects on a response variable cannot be distinguished from each other. Observational studies that try to study cause-and-effect will most likely fail due to confounding.
		</p><p>
			Experiments have a separate vocabulary because people who do experiments want to show that they're better than you. <strong>Treatments</strong> are the conditions applied to the individuals or collections of individuals (<strong>experimental units</strong> or <strong>subjects</strong> if units are human), which is a combination of specific values (<strong>levels</strong>) of the explanatory variables (<strong>factors</strong>).
		</p>
		<h4>Experimental design</h4>
		<p>
			In a lab environment, a simple design of applying the treatment to the experimental units, then measuring response variables would work well. However, outside of a lab, it would likely yield useless results due to confounding.
		</p><p>
			Good experimental design involves comparison, random assignment, control, and replication.
		</p><p>
			<strong>Comparison</strong>, well, compares two or more treatments.
		</p><p>
			<strong>Random assignment</strong> assigns experimental units to treatments using a chance process, which removes bias, similar to random selection in sampling. Random assignment should create groups of roughly equal sizes and balance out other factors, ensuring that effects of other variables are spread evenly among all groups.
		</p><p>
			<strong>Control</strong> ensures that certain variables are kept constant across groups, so the groups are alike except for the treatment, thus controlling the effects of those variables. Control helps prevent confounding and reduceds variability in the response variable.
		</p><p>
			<strong>Replication</strong> means using enough experimental units in each group to distinguish the effects of the treatments from chance variation between groups due to random assignment. Replication is used differently in statistics than the typical definition of "repeatability".
		</p><p>
			A <strong>control group</strong> provides a baseline for comparison between treatments. Some studies choose to have the control group receive inactive treatments, some choose an active treatments, while others choose to not have a control group (strictly to compare different treatments).
		</p><p>
			<strong>Placebo-controlled</strong> experiments assign inactive treatments to the control group, due to the <strong>placebo effect</strong>, a strong effect where subjects will often respond favorably to any treatment, inactive or not.
		</p><p>
			<strong>Double-blind</strong> experiments are preferred for experiments with huamn subjects, where neither the subjects nor those who interact with them and measure the response variable know which treatment a subject received. This eliminates possible bias introduced by the placebo effect or experimenters' expectations. In cases where an experiment cannot be double-blind, <strong>single-blind</strong> experiments still work by having either the subjects or the experimenters unaware of the treatments.
		</p>
		<h4>Completely randomized design</h4>
		<p>
			A <strong>completely randomized design</strong> assigns units to treatments completely by chance.
		</p>
		<img>
		<h4>Randomized block design</h4>
		<p>
			A <strong>randomized block design</strong> works similar to a stratified random sample, in that experimental units are divided into groups of similar units (<strong>blocks</strong>), with random assignment done separately within each block. This design accounts for the variation in response due to the blocking variable.
		</p><p>
			A good strategy is to control what you can, block on what you can't control, and randomize to create comparable groups.
		</p>
		<img>
		<p>
			<strong>Matched pairs design</strong> is a type of randomized block design for comparing two treatments, where similar units are formed into pairs, and random assignment done within each pair. Sometimes, a pair consists just of a single unit (which serves as its own control), and the order of the treatments is randomly assigned.
		</p>
		
		<h3 id = "applied_statistics_data-collection_misuse">Misuse</h3>
		<p>
			Chance variation determines the scope of inference. Inference about the population can be made iff the individuals are randomly selected from the population. Inference about cause-and-effect can be made iff the individuals are randomly assigned to groups.
		</p><p>
			Lack of realism in an experiment can prevent us from generalizing its results.
		</p><p>
			In the case that it is unpractical or unethical to do an experiment, we can still establish causation under the following conditions: strong association, consistent association, association of larger values of the explanatory variable with stronger responses, and a plausible alleged cause which precedes the effect in time.
		</p>
		<p>
			Data ethics:<br>
			An <strong>institutional review board</strong> must review all planned studies in advance, to protect the well-being of the subjects.<br>
			<strong>Informed consent</strong> is required from all subjects before data is collected.<br>
			<strong>Confidentiality</strong> is required for all individual data (different from anonymity, which is rare and difficult due to nonresponse); only statistical summaries for groups may be made public.
		</p>
	</body>
</html>
